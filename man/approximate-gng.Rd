% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/gng.R
\name{ApproximateGNG}
\alias{ApproximateGNG}
\title{Constructor of Approximate GrowingNeuralGas object}
\usage{
OptimizedGNG(x = NULL, labels = c(), beta = 0.99, alpha = 0.5,
  max.nodes = 1000, eps.n = 6e-04, eps.w = 0.05, max.edge.age = 200,
  train.online = FALSE, max.iter = 200, dim = 0,
  min.improvement = 0.001, lambda = 200, verbosity = 0, seed = -1,
  max_links = 16, efConstruction = 32, efSearch = 16, nsw = FALSE, recall = FALSE)
}
\arguments{
\item{x}{Passed data (matrix of data.frame) for offline training}

\item{labels}{Every example can be associated with labels that are added to nodes later. By default empty}

\item{beta}{Decrease the error variables of all node
nodes by this fraction (forgetting rate). Default 0.99}

\item{alpha}{Decrease the error variables of the nodes neighboring to
the newly inserted node by this fraction. Default 0.5}

\item{max.nodes}{Maximum number of nodes
(after reaching this size it will continue running, but new noes won't be added)}

\item{eps.n}{Strength of adaptation of neighbour node. Default \code{0.0006}}

\item{eps.w}{Strength of adaptation of winning node. Default \code{0.05}}

\item{max.edge.age}{Maximum edge age. Decrease to increase speed of change of graph topology. Default \code{200}}

\item{train.online}{If used will run in online fashion. Default \code{FALSE}}

\item{max.iter}{If training offline will stop if exceedes max.iter iterations. Default \code{200}}

\item{dim}{Used for training online, specifies dataset example dimensionality}

\item{min.improvement}{Used for offline (default) training.
Controls stopping criterion, decrease if training stops too early. Default \code{1e-3}}

\item{lambda}{New vertex is added every lambda iterations. Default 200}

\item{verbosity}{How verbose should the process be, as integer from \eqn{[0,6]}, default: \code{0}}

\item{seed}{Seed for internal randomization}

\item{max_links}{Defines number of links created for a new node of the HNSW graph upon insertion. \eqn{[2,100]} is a reasonable range. defining it to be higher works best on datasets with high intrinsic dimensionality or when high recall is needed.}

\item{efConstruction}{Can be seen as controlling the trade-off between index construction time and index quality of the HNSW. Should be increased until we achieve at least 95\% construction recall.}

\item{efSearch}{Higher efSearch translates to  better search recall but slower search times. Controls how approximate we can allow the ANN step, and consequently the GNG model to be in exchange for GNG construction time.}

\item{nsw}{Allows to constrict the HNSW to just 1 level, making it behave like the NSW algorithm.}

\item{recall}{Tells algorithm to calculate search recall during the process. Severely slows down algorithm. Useful for performance testing and debugging.}
}
\description{
Construct Approximate GNG object. To be used with high dimensions or high model size.
Substitutes the NN step of the GNG by an Approximate NN method: HNSW, several parameters are added to configure HNSW.
}
\examples{
\dontrun{

# Train ApproximateGNG offline:

    X <- gng.preset.sphere(1000)

    # Creating a 100 node model over 100 * lambda iterations of the GNG
    gng <- ApproximateGNG(X, max.nodes=100, max.iter=100, max_links = 16, efSearch = 16, efConstruction = 32)

    # Confirm construction recal ~= 0.95, sampling 100 nodes to do this:
    gng$getConstructionRecall(100)



# Train online:

    X <- gng.preset.sphere(1000)
    gng <- ApproximateGNG(train.online = TRUE, dim=3, max.nodes=100, max_links = 16, efSearch = 16, efConstruction = 32)
    insertExamples(gng, X)
    run(gng)
    Sys.sleep(10)
    pause(gng)
}
}

